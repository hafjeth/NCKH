"""
Knowledge Ingestion Engine
N·∫°p d·ªØ li·ªáu t·ª´ processed text v√†o ChromaDB v·ªõi OpenAI embeddings
"""

import os
from pathlib import Path
from typing import List
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document

# ==========================
# CONFIGURATION
# ==========================
PROCESSED_TEXT_DIR = "data/processed_text"
CHROMA_DB_DIR = "data/chroma_db"
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "YOUR_OPENAI_API_KEY_HERE")

# Chunking config
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200

# Embedding model
EMBEDDING_MODEL = "text-embedding-3-small"


# ==========================
# 1) ƒê·ªåC T·∫§T C·∫¢ FILE TEXT
# ==========================
def load_documents_from_directory(directory: str) -> List[Document]:
    """
    ƒê·ªçc t·∫•t c·∫£ file .txt t·ª´ th∆∞ m·ª•c processed_text.
    
    Args:
        directory: ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a file text
        
    Returns:

        List c√°c Document objects
    """
    documents = []
    text_dir = Path(directory)
    
    if not text_dir.exists():
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c: {directory}")
        return documents
    
    txt_files = list(text_dir.glob("*.txt"))
    
    if not txt_files:
        print(f"‚ö†Ô∏è Kh√¥ng c√≥ file .txt n√†o trong {directory}")
        return documents
    
    print(f"üìÇ T√¨m th·∫•y {len(txt_files)} file text")
    
    for txt_file in txt_files:
        try:
            with open(txt_file, "r", encoding="utf-8") as f:
                content = f.read()
            
            if content.strip():
                # T·∫°o Document v·ªõi metadata
                doc = Document(
                    page_content=content,
                    metadata={
                        "source": txt_file.name,
                        "file_path": str(txt_file),
                        "length": len(content)
                    }
                )
                documents.append(doc)
                print(f"  ‚úì ƒê·ªçc: {txt_file.name} ({len(content):,} chars)")
            else:
                print(f"  ‚ö†Ô∏è B·ªè qua file r·ªóng: {txt_file.name}")
                
        except Exception as e:
            print(f"  ‚ùå L·ªói ƒë·ªçc {txt_file.name}: {e}")
            continue
    
    print(f"\n‚úì ƒê√£ ƒë·ªçc {len(documents)} documents\n")
    return documents


# ==========================
# 2) CHUNKING V·ªöI LANGCHAIN
# ==========================
def chunk_documents(documents: List[Document]) -> List[Document]:
    """
    Chia documents th√†nh chunks nh·ªè h∆°n.
    
    Args:
        documents: List c√°c Document c·∫ßn chia
        
    Returns:
        List c√°c Document chunks
    """
    print(f"‚úÇÔ∏è B·∫Øt ƒë·∫ßu chunking v·ªõi chunk_size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP}")
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    
    chunks = text_splitter.split_documents(documents)
    
    # Th√™m metadata v·ªÅ chunk
    for i, chunk in enumerate(chunks):
        chunk.metadata["chunk_id"] = i
        chunk.metadata["chunk_size"] = len(chunk.page_content)
    
    print(f"‚úì ƒê√£ t·∫°o {len(chunks)} chunks\n")
    return chunks


# ==========================
# 3) KH·ªûI T·∫†O EMBEDDING MODEL
# ==========================
def initialize_embeddings() -> OpenAIEmbeddings:
    """
    Kh·ªüi t·∫°o OpenAI Embeddings model.
    
    Returns:
        OpenAIEmbeddings object
    """
    # Set API key n·∫øu ch∆∞a c√≥ trong env
    if OPENAI_API_KEY and OPENAI_API_KEY != "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":
        os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
    
    print(f"üîë Kh·ªüi t·∫°o embedding model: {EMBEDDING_MODEL}")
    
    embeddings = OpenAIEmbeddings(
        model=EMBEDDING_MODEL,
        openai_api_key=OPENAI_API_KEY
    )
    
    print("‚úì Embedding model ƒë√£ s·∫µn s√†ng\n")
    return embeddings


# ==========================
# 4) L∆ØU V√ÄO CHROMADB
# ==========================
def save_to_chromadb(chunks: List[Document], embeddings: OpenAIEmbeddings) -> Chroma:
    """
    L∆∞u chunks v√†o ChromaDB v·ªõi embeddings.
    
    Args:
        chunks: List c√°c Document chunks
        embeddings: Embedding model
        
    Returns:
        Chroma vectorstore object
    """
    print(f"üíæ ƒêang l∆∞u {len(chunks)} chunks v√†o ChromaDB...")
    print(f"üìÅ Persist directory: {CHROMA_DB_DIR}\n")
    
    # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
    os.makedirs(CHROMA_DB_DIR, exist_ok=True)
    
    try:
        # Kh·ªüi t·∫°o ho·∫∑c load existing ChromaDB
        vectorstore = Chroma(
            persist_directory=CHROMA_DB_DIR,
            embedding_function=embeddings,
            collection_name="knowledge_base"
        )
        
        # Th√™m documents v√†o vectorstore
        # Chia nh·ªè ƒë·ªÉ tr√°nh timeout v·ªõi batch l·ªõn
        batch_size = 50
        total_batches = (len(chunks) + batch_size - 1) // batch_size
        
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i+batch_size]
            batch_num = i // batch_size + 1
            
            print(f"  üì¶ Processing batch {batch_num}/{total_batches} ({len(batch)} chunks)...")
            vectorstore.add_documents(batch)
        
        print(f"\n‚úì ƒê√£ l∆∞u th√†nh c√¥ng {len(chunks)} chunks v√†o ChromaDB!")
        print(f"‚úì Collection: knowledge_base")
        print(f"‚úì Location: {os.path.abspath(CHROMA_DB_DIR)}\n")
        
        return vectorstore
        
    except Exception as e:
        print(f"\n‚ùå L·ªói khi l∆∞u v√†o ChromaDB: {e}")
        raise


# ==========================
# 5) VERIFY DATABASE
# ==========================
def verify_database(vectorstore: Chroma):
    """
    Ki·ªÉm tra xem database ƒë√£ ƒë∆∞·ª£c t·∫°o ƒë√∫ng ch∆∞a.
    
    Args:
        vectorstore: Chroma vectorstore object
    """
    print("üîç ƒêang verify database...")
    
    try:
        # L·∫•y collection
        collection = vectorstore._collection
        count = collection.count()
        
        print(f"‚úì Database verification:")
        print(f"  - Total vectors: {count:,}")
        print(f"  - Collection name: {collection.name}")
        
        # Test query
        if count > 0:
            print(f"\nüß™ Test query...")
            results = vectorstore.similarity_search("test", k=1)
            if results:
                print(f"‚úì Query th√†nh c√¥ng! Sample result:")
                print(f"  - Source: {results[0].metadata.get('source', 'N/A')}")
                print(f"  - Content preview: {results[0].page_content[:100]}...")
        
        print("\n‚úÖ Database ƒë√£ s·∫µn s√†ng s·ª≠ d·ª•ng!")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Verify warning: {e}")


# ==========================
# 6) MAIN PIPELINE
# ==========================
def main():
    """
    Main function - ch·∫°y to√†n b·ªô pipeline ingestion.
    """
    print("="*70)
    print("üöÄ KNOWLEDGE INGESTION PIPELINE")
    print("="*70)
    print()
    
    # Step 1: Load documents
    print("üìñ STEP 1: Loading documents...")
    print("-" * 70)
    documents = load_documents_from_directory(PROCESSED_TEXT_DIR)
    
    if not documents:
        print("‚ùå Kh√¥ng c√≥ documents ƒë·ªÉ x·ª≠ l√Ω. Tho√°t.")
        return
    
    # Step 2: Chunk documents
    print("‚úÇÔ∏è STEP 2: Chunking documents...")
    print("-" * 70)
    chunks = chunk_documents(documents)
    
    # Step 3: Initialize embeddings
    print("üîë STEP 3: Initializing embeddings...")
    print("-" * 70)
    
    if not OPENAI_API_KEY or OPENAI_API_KEY == "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":
        print("‚ùå CH∆ØA C√ì OPENAI_API_KEY!")
        print("   Vui l√≤ng:")
        print("   1. Set env: export OPENAI_API_KEY=sk-...")
        print("   2. Ho·∫∑c s·ª≠a trong code: OPENAI_API_KEY = 'sk-...'")
        return
    
    embeddings = initialize_embeddings()
    
    # Step 4: Save to ChromaDB
    print("üíæ STEP 4: Saving to ChromaDB...")
    print("-" * 70)
    vectorstore = save_to_chromadb(chunks, embeddings)
    
    # Step 5: Verify
    print("üîç STEP 5: Verifying database...")
    print("-" * 70)
    verify_database(vectorstore)
    
    print("\n" + "="*70)
    print("‚úÖ INGESTION HO√ÄN T·∫§T!")
    print("="*70)
    print(f"\nüìä Summary:")
    print(f"  - Documents processed: {len(documents)}")
    print(f"  - Total chunks: {len(chunks)}")
    print(f"  - Database location: {os.path.abspath(CHROMA_DB_DIR)}")
    print(f"  - Embedding model: {EMBEDDING_MODEL}")
    print()


# ==========================
# 7) RUN
# ==========================
if __name__ == "__main__":
    main()