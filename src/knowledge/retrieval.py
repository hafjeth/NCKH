import sys
from pathlib import Path
from typing import List, Dict, Optional
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
import logging

# Setup logging with UTF-8 encoding
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('retrieval.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
# Force UTF-8 for console output
if sys.stdout.encoding != 'utf-8':
    sys.stdout.reconfigure(encoding='utf-8')

logger = logging.getLogger(__name__)


class RetrievalSystem:
    """
    H·ªá th·ªëng truy xu·∫•t th√¥ng tin t·ª´ ChromaDB
    Nh·∫≠n query, tr·∫£ v·ªÅ top-k documents li√™n quan nh·∫•t
    """
    
    def __init__(
        self,
        chroma_db_dir: str = "data/chroma_db",
        collection_name: str = "knowledge_base",
        embedding_model: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        top_k: int = 3
    ):
        """
        Kh·ªüi t·∫°o Retrieval System
        
        Args:
            chroma_db_dir: Th∆∞ m·ª•c ch·ª©a ChromaDB
            collection_name: T√™n collection
            embedding_model: Model embedding (ph·∫£i gi·ªëng v·ªõi l√∫c ingest)
            top_k: S·ªë l∆∞·ª£ng documents tr·∫£ v·ªÅ (m·∫∑c ƒë·ªãnh 3)
        """
        self.chroma_db_dir = Path(chroma_db_dir)
        self.collection_name = collection_name
        self.top_k = top_k
        
        # Load embedding model
        logger.info(f"Loading embedding model: {embedding_model}")
        self.embedding_model = SentenceTransformer(embedding_model)
        
        # Connect to ChromaDB
        logger.info(f"Connecting to ChromaDB at: {self.chroma_db_dir}")
        self.client = chromadb.PersistentClient(
            path=str(self.chroma_db_dir),
            settings=Settings(anonymized_telemetry=False)
        )
        
        # Get collection
        try:
            self.collection = self.client.get_collection(name=self.collection_name)
            logger.info(f"Connected to collection: {self.collection_name}")
            logger.info(f"Total documents in collection: {self.collection.count()}")
        except Exception as e:
            logger.error(f"Failed to load collection '{self.collection_name}': {str(e)}")
            raise
    
    def retrieve(
        self, 
        query: str, 
        top_k: Optional[int] = None,
        filter_metadata: Optional[Dict] = None
    ) -> List[Dict]:
        """
        Truy xu·∫•t top-k documents li√™n quan nh·∫•t v·ªõi query
        
        Args:
            query: C√¢u h·ªèi/query t·ª´ ng∆∞·ªùi d√πng
            top_k: S·ªë l∆∞·ª£ng documents tr·∫£ v·ªÅ (None = d√πng default)
            filter_metadata: ƒêi·ªÅu ki·ªán filter (VD: {'filename': 'abc.txt'})
        
        Returns:
            List c√°c documents v·ªõi metadata v√† score
        """
        if not query or not query.strip():
            logger.warning("Empty query received")
            return []
        
        k = top_k if top_k is not None else self.top_k
        
        try:
            # Generate query embedding
            logger.info(f"Processing query: '{query}'")
            query_embedding = self.embedding_model.encode([query]).tolist()
            
            # Search in ChromaDB
            results = self.collection.query(
                query_embeddings=query_embedding,
                n_results=k,
                where=filter_metadata  # Filter n·∫øu c√≥
            )
            
            # Format results
            retrieved_docs = []
            
            if results['documents'] and results['documents'][0]:
                for i, (doc, meta, distance) in enumerate(zip(
                    results['documents'][0],
                    results['metadatas'][0],
                    results['distances'][0]
                ), 1):
                    retrieved_docs.append({
                        'rank': i,
                        'content': doc,
                        'metadata': meta,
                        'distance': distance,
                        'similarity_score': self._distance_to_similarity(distance)
                    })
                
                logger.info(f"Retrieved {len(retrieved_docs)} documents")
            else:
                logger.warning("No documents found for the query")
            
            return retrieved_docs
            
        except Exception as e:
            logger.error(f"Error during retrieval: {str(e)}")
            return []
    
    def _distance_to_similarity(self, distance: float) -> float:
        """
        Chuy·ªÉn ƒë·ªïi distance th√†nh similarity score (0-1)
        Distance c√†ng nh·ªè => Similarity c√†ng cao
        
        Args:
            distance: Distance t·ª´ ChromaDB (L2 distance)
        
        Returns:
            Similarity score (0-1), 1 l√† gi·ªëng nh·∫•t
        """
        # S·ª≠ d·ª•ng c√¥ng th·ª©c: similarity = 1 / (1 + distance)
        return 1.0 / (1.0 + distance)
    
    def retrieve_with_context(
        self,
        query: str,
        top_k: Optional[int] = None,
        context_window: int = 1
    ) -> List[Dict]:
        """
        Truy xu·∫•t documents k√®m context (chunks tr∆∞·ªõc/sau)
        
        Args:
            query: C√¢u h·ªèi
            top_k: S·ªë documents tr·∫£ v·ªÅ
            context_window: S·ªë chunks tr∆∞·ªõc/sau c·∫ßn l·∫•y th√™m
        
        Returns:
            List documents v·ªõi context m·ªü r·ªông
        """
        # L·∫•y k·∫øt qu·∫£ th√¥ng th∆∞·ªùng tr∆∞·ªõc
        base_results = self.retrieve(query, top_k)
        
        if not base_results or context_window == 0:
            return base_results
        
        # M·ªü r·ªông context cho m·ªói result
        enhanced_results = []
        
        for result in base_results:
            filename = result['metadata']['filename']
            chunk_id = result['metadata']['chunk_id']
            
            # L·∫•y c√°c chunks l√¢n c·∫≠n
            context_chunks = self._get_context_chunks(
                filename, 
                chunk_id, 
                context_window
            )
            
            result['context_before'] = context_chunks['before']
            result['context_after'] = context_chunks['after']
            result['full_context'] = (
                '\n'.join(context_chunks['before']) + 
                '\n' + result['content'] + '\n' + 
                '\n'.join(context_chunks['after'])
            )
            
            enhanced_results.append(result)
        
        return enhanced_results
    
    def _get_context_chunks(
        self,
        filename: str,
        chunk_id: int,
        window: int
    ) -> Dict[str, List[str]]:
        """
        L·∫•y c√°c chunks l√¢n c·∫≠n c·ªßa m·ªôt chunk
        
        Args:
            filename: T√™n file
            chunk_id: ID c·ªßa chunk hi·ªán t·∫°i
            window: S·ªë chunks tr∆∞·ªõc/sau c·∫ßn l·∫•y
        
        Returns:
            Dict v·ªõi 'before' v√† 'after' chunks
        """
        context = {'before': [], 'after': []}
        
        try:
            # L·∫•y chunks tr∆∞·ªõc
            for i in range(chunk_id - window, chunk_id):
                if i >= 0:
                    chunk_results = self.collection.get(
                        ids=[f"{Path(filename).stem}_chunk_{i}"],
                        include=['documents']
                    )
                    if chunk_results['documents']:
                        context['before'].append(chunk_results['documents'][0])
            
            # L·∫•y chunks sau
            for i in range(chunk_id + 1, chunk_id + window + 1):
                chunk_results = self.collection.get(
                    ids=[f"{Path(filename).stem}_chunk_{i}"],
                    include=['documents']
                )
                if chunk_results['documents']:
                    context['after'].append(chunk_results['documents'][0])
        
        except Exception as e:
            logger.warning(f"Error getting context chunks: {str(e)}")
        
        return context
    
    def retrieve_by_filename(
        self,
        filename: str,
        top_k: Optional[int] = None
    ) -> List[Dict]:
        """
        L·∫•y t·∫•t c·∫£ chunks t·ª´ m·ªôt file c·ª• th·ªÉ
        
        Args:
            filename: T√™n file c·∫ßn l·∫•y
            top_k: Gi·ªõi h·∫°n s·ªë chunks (None = l·∫•y t·∫•t c·∫£)
        
        Returns:
            List c√°c chunks t·ª´ file ƒë√≥
        """
        try:
            results = self.collection.get(
                where={"filename": filename},
                limit=top_k if top_k else 10000,
                include=['documents', 'metadatas']
            )
            
            chunks = []
            if results['documents']:
                for doc, meta in zip(results['documents'], results['metadatas']):
                    chunks.append({
                        'content': doc,
                        'metadata': meta
                    })
                
                # S·∫Øp x·∫øp theo chunk_id
                chunks.sort(key=lambda x: x['metadata']['chunk_id'])
                logger.info(f"Retrieved {len(chunks)} chunks from '{filename}'")
            
            return chunks
            
        except Exception as e:
            logger.error(f"Error retrieving from file '{filename}': {str(e)}")
            return []
    
    def get_all_filenames(self) -> List[str]:
        """
        L·∫•y danh s√°ch t·∫•t c·∫£ filenames trong database
        
        Returns:
            List t√™n files
        """
        try:
            # L·∫•y sample ƒë·ªÉ extract filenames
            results = self.collection.get(
                limit=10000,
                include=['metadatas']
            )
            
            filenames = set()
            if results['metadatas']:
                for meta in results['metadatas']:
                    filenames.add(meta.get('filename', ''))
            
            return sorted(list(filenames))
        
        except Exception as e:
            logger.error(f"Error getting filenames: {str(e)}")
            return []
    
    def get_stats(self) -> Dict:
        """
        L·∫•y th·ªëng k√™ v·ªÅ database
        
        Returns:
            Dict ch·ª©a th√¥ng tin th·ªëng k√™
        """
        try:
            total_docs = self.collection.count()
            filenames = self.get_all_filenames()
            
            return {
                'total_documents': total_docs,
                'total_files': len(filenames),
                'collection_name': self.collection_name,
                'embedding_dimension': self.embedding_model.get_sentence_embedding_dimension(),
                'sample_files': filenames[:10]
            }
        except Exception as e:
            logger.error(f"Error getting stats: {str(e)}")
            return {}
    
    def format_results_for_display(
        self,
        results: List[Dict],
        show_metadata: bool = True,
        max_content_length: int = 300
    ) -> str:
        """
        Format k·∫øt qu·∫£ retrieval ƒë·ªÉ hi·ªÉn th·ªã ƒë·∫πp
        
        Args:
            results: List k·∫øt qu·∫£ t·ª´ retrieve()
            show_metadata: C√≥ hi·ªÉn th·ªã metadata kh√¥ng
            max_content_length: ƒê·ªô d√†i t·ªëi ƒëa c·ªßa content hi·ªÉn th·ªã
        
        Returns:
            String formatted ƒë·ªÉ print
        """
        if not results:
            return "Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ n√†o."
        
        output = []
        output.append("=" * 80)
        output.append(f"RETRIEVAL RESULTS - Found {len(results)} documents")
        output.append("=" * 80)
        
        for result in results:
            output.append(f"\nüìÑ Rank {result['rank']}")
            output.append(f"üìä Similarity Score: {result['similarity_score']:.4f}")
            
            if show_metadata:
                meta = result['metadata']
                output.append(f"üìÅ File: {meta.get('filename', 'N/A')}")
                output.append(f"üî¢ Chunk: {meta.get('chunk_id', 'N/A')}/{meta.get('total_chunks', 'N/A')}")
            
            # Content (truncate if too long)
            content = result['content']
            if len(content) > max_content_length:
                content = content[:max_content_length] + "..."
            
            output.append(f"\nüìù Content:\n{content}")
            output.append("-" * 80)
        
        return "\n".join(output)


def main():
    """
    Demo s·ª≠ d·ª•ng Retrieval System
    """
    # Kh·ªüi t·∫°o retrieval system
    project_root = Path(__file__).parent.parent.parent
    
    retriever = RetrievalSystem(
        chroma_db_dir=str(project_root / "data/chroma_db"),
        collection_name="knowledge_base",
        top_k=3
    )
    
    # Hi·ªÉn th·ªã stats
    print("\n" + "=" * 80)
    print("DATABASE STATISTICS")
    print("=" * 80)
    stats = retriever.get_stats()
    for key, value in stats.items():
        if key == 'sample_files':
            print(f"\n{key}:")
            for f in value:
                print(f"  - {f}")
        else:
            print(f"{key}: {value}")
    
    # Test queries
    test_queries = [
        "CBAM l√† g√¨?",
        "Quy ƒë·ªãnh v·ªÅ ph√°t th·∫£i kh√≠ nh√† k√≠nh",
        "Ngh·ªã ƒë·ªãnh v·ªÅ b·∫£o v·ªá m√¥i tr∆∞·ªùng",
        "Chuy·ªÉn ƒë·ªïi xanh ng√†nh d·ªát may",
        "Industry 4.0"
    ]
    
    print("\n" + "=" * 80)
    print("TESTING RETRIEVAL WITH SAMPLE QUERIES")
    print("=" * 80)
    
    for query in test_queries:
        print(f"\n\n{'='*80}")
        print(f"üîç QUERY: {query}")
        print(f"{'='*80}")
        
        # Retrieve
        results = retriever.retrieve(query, top_k=3)
        
        # Display
        formatted_output = retriever.format_results_for_display(
            results,
            show_metadata=True,
            max_content_length=200
        )
        print(formatted_output)
        
        # Th√™m d√≤ng ph√¢n c√°ch
        print("\n" + "‚îÄ" * 80)


if __name__ == "__main__":
    main()